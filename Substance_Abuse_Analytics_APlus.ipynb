{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# Global Substance Abuse Mortality Analytics (1990–2024)\n",
    "\n",
    "**Author:** Udodirim | **Portfolio Project** |  \n",
    "**Dataset:** Global deaths attributable to substance abuse (drug, alcohol, tobacco) — 207 countries, 1990–2019  \n",
    "**Tools:** Python · SARIMAX · Prophet · AutoTS · Power BI · Tableau\n",
    "\n",
    "---\n",
    "\n",
    "## Analytical Objectives\n",
    "\n",
    "1. **Understand** historical trends in substance-related mortality across countries and continents (1990–2019)\n",
    "2. **Compare** direct vs risk-attributed deaths across five cause categories\n",
    "3. **Identify** the highest-burden geographies and how patterns have shifted over 30 years\n",
    "4. **Forecast** mortality trajectories through 2024 using three competing time-series models\n",
    "5. **Export** analysis-ready datasets for Power BI and Tableau dashboards\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "| Section | Description |\n",
    "|---|---|\n",
    "| 0 | GitHub Setup |\n",
    "| 1 | Environment Setup |\n",
    "| 2 | Data Loading |\n",
    "| 3 | Data Quality Checks |\n",
    "| 4 | Data Dictionary |\n",
    "| 5 | Standardisation |\n",
    "| 6 | KPIs |\n",
    "| 7 | EDA Visualisations |\n",
    "| 8 | Feature Engineering |\n",
    "| 9 | Modelling & Forecasting |\n",
    "| 10 | Exports (Power BI / Tableau) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-github",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 0 — GitHub Repository Setup\n",
    "\n",
    "Run this cell **once** at the start of each Colab session.  \n",
    "It creates the remote repo (first time only), configures git identity, and clones/links the repo so subsequent commit cells just work.\n",
    "\n",
    "> **Prerequisites:** Generate a GitHub Personal Access Token (PAT) with `repo` scope at  \n",
    "> `GitHub → Settings → Developer Settings → Personal Access Tokens → Tokens (classic)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-github-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./substance-abuse-analytics/venv/lib/python3.13/site-packages (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Repo already exists: https://github.com/Udodirim/substance-abuse-analytics\n",
      "✓ Repo already initialised — pulled latest changes.\n",
      "\n",
      "✓ Git identity configured for Udodirim <udynwosu@gmail.com>\n",
      "✓ Working directory: /Users/udodirimnwosu/Desktop/Python Projects/Git-hub Project Files/substance-abuse-analytics\n",
      "\n",
      " GitHub setup complete. Commit cells below will push to this repo.\n"
     ]
    }
   ],
   "source": [
    "# [0] GITHUB SETUP — create repo via API, configure git, link local folder\n",
    "# Run once per session. Safe to re-run (idempotent).\n",
    "\n",
    "\n",
    "import subprocess, sys\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\"], check=True)\n",
    "\n",
    "import os, subprocess, requests, json\n",
    "from pathlib import Path\n",
    "\n",
    "# ── USER CONFIG ───────────────────────────────────────────────────────────────\n",
    "GITHUB_USERNAME  = \"Udodirim\"\n",
    "GITHUB_EMAIL     = \"udynwosu@gmail.com\"\n",
    "REPO_NAME        = \"substance-abuse-analytics\"\n",
    "REPO_DESCRIPTION = \"Global substance abuse mortality analytics (1990–2019) with SARIMAX/Prophet forecasting. Portfolio project — Eden Mandate AI_Tools.\"\n",
    "REPO_PRIVATE     = False\n",
    "\n",
    "# ── LOCAL PROJECT PATH (VSCode — update to where your data/notebook lives) ───\n",
    "PROJECT_DIR = Path(r\"/Users/udodirimnwosu/Desktop/Python Projects/Git-hub Project Files/substance-abuse-analytics\")  # ← update this\n",
    "REPO_DIR    = PROJECT_DIR  # repo lives IN your project folder\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Read token from environment variable (set once in terminal — see instructions below)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "GITHUB_TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# 1) Create repo on GitHub if it doesn't exist yet\n",
    "check = requests.get(f\"https://api.github.com/repos/{GITHUB_USERNAME}/{REPO_NAME}\", headers=HEADERS)\n",
    "if check.status_code == 200:\n",
    "    print(f\"✓ Repo already exists: https://github.com/{GITHUB_USERNAME}/{REPO_NAME}\")\n",
    "elif check.status_code == 404:\n",
    "    payload = {\"name\": REPO_NAME, \"description\": REPO_DESCRIPTION,\n",
    "               \"private\": REPO_PRIVATE, \"auto_init\": True}\n",
    "    r = requests.post(\"https://api.github.com/user/repos\", headers=HEADERS, json=payload)\n",
    "    if r.status_code == 201:\n",
    "        print(f\"✓ Created repo: https://github.com/{GITHUB_USERNAME}/{REPO_NAME}\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Repo creation failed: {r.status_code} — {r.text}\")\n",
    "else:\n",
    "    raise RuntimeError(f\"GitHub API error: {check.status_code} — {check.text}\")\n",
    "\n",
    "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 2) Initialise git in project folder (or pull if already initialised)\n",
    "git_dir = REPO_DIR / \".git\"\n",
    "if not git_dir.exists():\n",
    "    subprocess.run([\"git\", \"init\"],                                   cwd=REPO_DIR, check=True)\n",
    "    subprocess.run([\"git\", \"remote\", \"add\", \"origin\", REPO_URL],     cwd=REPO_DIR, check=True)\n",
    "    subprocess.run([\"git\", \"fetch\", \"origin\"],                        cwd=REPO_DIR, capture_output=True)\n",
    "    # Try to checkout main branch if it exists remotely\n",
    "    result = subprocess.run([\"git\", \"checkout\", \"-b\", \"main\", \"--track\", \"origin/main\"],\n",
    "                             cwd=REPO_DIR, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        subprocess.run([\"git\", \"checkout\", \"-b\", \"main\"], cwd=REPO_DIR, capture_output=True)\n",
    "    print(f\"✓ Git initialised in: {REPO_DIR}\")\n",
    "else:\n",
    "    subprocess.run([\"git\", \"remote\", \"set-url\", \"origin\", REPO_URL], cwd=REPO_DIR, capture_output=True)\n",
    "    subprocess.run([\"git\", \"pull\", \"origin\", \"main\"],                 cwd=REPO_DIR, capture_output=True)\n",
    "    print(f\"✓ Repo already initialised — pulled latest changes.\")\n",
    "\n",
    "# 3) Configure git identity\n",
    "subprocess.run([\"git\", \"config\", \"user.name\",  GITHUB_USERNAME], cwd=REPO_DIR)\n",
    "subprocess.run([\"git\", \"config\", \"user.email\", GITHUB_EMAIL],    cwd=REPO_DIR)\n",
    "\n",
    "print(f\"\\n✓ Git identity configured for {GITHUB_USERNAME} <{GITHUB_EMAIL}>\")\n",
    "print(f\"✓ Working directory: {REPO_DIR}\")\n",
    "print(\"\\n GitHub setup complete. Commit cells below will push to this repo.\")\n",
    "\n",
    "\n",
    "# ── SHARED COMMIT HELPER (used by all subsequent commit cells) ────────────────\n",
    "def git_commit_push(message: str, files: list = None):\n",
    "    \"\"\"\n",
    "    Stage, commit, and push to GitHub.\n",
    "    Automatically includes the current notebook.\n",
    "    Call this at the end of each major section.\n",
    "    \"\"\"\n",
    "    # Stage specific files or everything\n",
    "    if files:\n",
    "        for f in files:\n",
    "            subprocess.run([\"git\", \"add\", str(f)], cwd=REPO_DIR, capture_output=True)\n",
    "    else:\n",
    "        subprocess.run([\"git\", \"add\", \"-A\"], cwd=REPO_DIR, capture_output=True)\n",
    "\n",
    "    # Commit (skip gracefully if nothing changed)\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"commit\", \"-m\", message],\n",
    "        cwd=REPO_DIR, capture_output=True, text=True\n",
    "    )\n",
    "    if \"nothing to commit\" in result.stdout:\n",
    "        print(\"ℹ  No changes to commit.\")\n",
    "        return\n",
    "\n",
    "    # Push\n",
    "    push = subprocess.run(\n",
    "        [\"git\", \"push\", \"origin\", \"main\"],\n",
    "        cwd=REPO_DIR, capture_output=True, text=True\n",
    "    )\n",
    "    if push.returncode == 0:\n",
    "        print(f\" Pushed: '{message}'\")\n",
    "        print(f\"   → https://github.com/{GITHUB_USERNAME}/{REPO_NAME}\")\n",
    "    else:\n",
    "        print(\" Push failed:\", push.stderr)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s1",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 — Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "/Users/udodirimnwosu/Desktop/Python Projects/Git-hub Project Files/substance-abuse-analytics/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete ✓  |  Prophet: True  |  AutoTS: True\n",
      "Project dir : /Users/udodirimnwosu/Desktop/Python Projects/Git-hub Project Files/substance-abuse-analytics\n",
      "Exports dir : /Users/udodirimnwosu/Desktop/Python Projects/Git-hub Project Files/substance-abuse-analytics/exports\n",
      "Figures dir : /Users/udodirimnwosu/Desktop/Python Projects/Git-hub Project Files/substance-abuse-analytics/exports/figures\n"
     ]
    }
   ],
   "source": [
    "# [1] ENVIRONMENT SETUP — make folders, install libs, set global options\n",
    "\n",
    "import subprocess, sys\n",
    "\n",
    "# ── Install dependencies ───────────────────────────────────────────────────────\n",
    "subprocess.run([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "    \"pandas\", \"numpy\", \"matplotlib\", \"tqdm\",\n",
    "    \"statsmodels\", \"pyarrow\", \"prophet\", \"autots\"\n",
    "], check=True)\n",
    "\n",
    "from pathlib import Path\n",
    "import os, warnings, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ── Project directories (VSCode — local path) ─────────────────────────────────\n",
    "PROJECT_DIR = Path(\"/Users/udodirimnwosu/Desktop/Python Projects/Git-hub Project Files/substance-abuse-analytics\")\n",
    "EXPORTS_DIR = PROJECT_DIR / \"exports\"\n",
    "FIG_DIR     = EXPORTS_DIR / \"figures\"\n",
    "for d in (PROJECT_DIR, EXPORTS_DIR, FIG_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ── Global settings ────────────────────────────────────────────────────────────\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "np.random.seed(42); random.seed(42)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.float_format\", \"{:,.2f}\".format)\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# ── Optional library availability flags ───────────────────────────────────────\n",
    "PROPHET_OK = True\n",
    "AUTOTS_OK  = True\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    import logging\n",
    "    logging.getLogger(\"prophet\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"cmdstanpy\").setLevel(logging.WARNING)\n",
    "except Exception as e:\n",
    "    PROPHET_OK = False\n",
    "    print(f\"Prophet not available: {e}\")\n",
    "try:\n",
    "    from autots import AutoTS\n",
    "except Exception as e:\n",
    "    AUTOTS_OK = False\n",
    "    print(f\"AutoTS not available: {e}\")\n",
    "\n",
    "print(f\"Setup complete ✓  |  Prophet: {PROPHET_OK}  |  AutoTS: {AUTOTS_OK}\")\n",
    "print(f\"Project dir : {PROJECT_DIR}\")\n",
    "print(f\"Exports dir : {EXPORTS_DIR}\")\n",
    "print(f\"Figures dir : {FIG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s2",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 — Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded : /Users/udodirimnwosu/Desktop/Python Projects/Git-hub Project Files/substance-abuse-analytics/_abuse.csv\n",
      "Shape  : (6210, 8)\n",
      "Memory : 0.494 MB\n",
      "\n",
      "Columns: ['continent', 'country', 'year', 'deaths_direct_drug', 'deaths_direct_alcohol', 'deaths_risk_tobacco', 'deaths_risk_drug', 'deaths_risk_alcohol']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>continent</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>deaths_direct_drug</th>\n",
       "      <th>deaths_direct_alcohol</th>\n",
       "      <th>deaths_risk_tobacco</th>\n",
       "      <th>deaths_risk_drug</th>\n",
       "      <th>deaths_risk_alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1990</td>\n",
       "      <td>93</td>\n",
       "      <td>72</td>\n",
       "      <td>9723</td>\n",
       "      <td>174</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Europe</td>\n",
       "      <td>Albania</td>\n",
       "      <td>1990</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3468</td>\n",
       "      <td>64</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Africa</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>1990</td>\n",
       "      <td>171</td>\n",
       "      <td>75</td>\n",
       "      <td>17985</td>\n",
       "      <td>296</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oceania</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>1990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Europe</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>1990</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  continent         country  year  deaths_direct_drug  deaths_direct_alcohol  \\\n",
       "0      Asia     Afghanistan  1990                  93                     72   \n",
       "1    Europe         Albania  1990                   5                     10   \n",
       "2    Africa         Algeria  1990                 171                     75   \n",
       "3   Oceania  American Samoa  1990                   0                      0   \n",
       "4    Europe         Andorra  1990                   0                      1   \n",
       "\n",
       "   deaths_risk_tobacco  deaths_risk_drug  deaths_risk_alcohol  \n",
       "0                 9723               174                  356  \n",
       "1                 3468                64                  289  \n",
       "2                17985               296                  496  \n",
       "3                   39                 1                    3  \n",
       "4                   62                 2                   22  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dtypes:\n",
      "continent                  str\n",
      "country                    str\n",
      "year                     int64\n",
      "deaths_direct_drug       int64\n",
      "deaths_direct_alcohol    int64\n",
      "deaths_risk_tobacco      int64\n",
      "deaths_risk_drug         int64\n",
      "deaths_risk_alcohol      int64\n",
      "dtype: object\n",
      "\n",
      "'year' column detected. Range: 1990 → 2019\n"
     ]
    }
   ],
   "source": [
    "# [2] LOAD DATA — read CSV from local folder and quick sanity printouts\n",
    "\n",
    "import os, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ── Update this to where your CSV file is saved locally ───────────────────────\n",
    "RAW_CSV_PATH = PROJECT_DIR / \"_abuse.csv\"  # ← put your CSV inside your project folder\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "assert os.path.exists(RAW_CSV_PATH), (\n",
    "    f\"CSV not found at: {RAW_CSV_PATH}\\n\"\n",
    "    f\"Make sure _abuse.csv is inside: {PROJECT_DIR}\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(RAW_CSV_PATH, engine=\"pyarrow\")\n",
    "except Exception:\n",
    "    df = pd.read_csv(RAW_CSV_PATH)\n",
    "\n",
    "print(\"Loaded :\", RAW_CSV_PATH)\n",
    "print(\"Shape  :\", df.shape)\n",
    "print(f\"Memory : {df.memory_usage(deep=True).sum() / 1e6:.3f} MB\")\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "display(df.head(5))\n",
    "print(\"\\nDtypes:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Auto-detect time axis\n",
    "if \"date\" in df.columns:\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    print(f\"\\n'date' column detected. Range: {df['date'].min()} → {df['date'].max()}\")\n",
    "elif \"year\" in df.columns or \"Year\" in df.columns:\n",
    "    yr = \"year\" if \"year\" in df.columns else \"Year\"\n",
    "    df[yr] = pd.to_numeric(df[yr], errors=\"coerce\")\n",
    "    print(f\"\\n'{yr}' column detected. Range: {int(df[yr].min())} → {int(df[yr].max())}\")\n",
    "else:\n",
    "    print(\"\\nNo 'date' or 'year' column detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s3",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 — Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-quality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nulls: 0\n",
      "  ✓ No nulls detected\n",
      "  ✓ No empty-string cells\n",
      "\n",
      "Exact duplicate rows: 0\n",
      "  ✓ No duplicate rows\n",
      "  ✓ No negative values in numeric columns\n",
      "\n",
      "Final shape: (6210, 8)\n"
     ]
    }
   ],
   "source": [
    "# [3] DATA QUALITY CHECKS — nulls, empties, duplicates, negative values\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "def strip_object_cols(df_):\n",
    "    for c in df_.select_dtypes(include=\"str\").columns:\n",
    "        df_[c] = df_[c].astype(str).str.strip()\n",
    "    return df_\n",
    "\n",
    "df = strip_object_cols(df)\n",
    "\n",
    "# Nulls\n",
    "null_by_col  = df.isna().sum()\n",
    "total_nulls  = int(null_by_col.sum())\n",
    "print(f\"Total nulls: {total_nulls}\")\n",
    "if total_nulls:\n",
    "    display(null_by_col[null_by_col > 0].sort_values(ascending=False))\n",
    "else:\n",
    "    print(\"  ✓ No nulls detected\")\n",
    "\n",
    "# Empty strings\n",
    "empty_like = (df.select_dtypes(include=\"str\") == \"\").sum()\n",
    "if (empty_like > 0).any():\n",
    "    print(\"\\nEmpty-string counts:\"); display(empty_like[empty_like > 0])\n",
    "else:\n",
    "    print(\"  ✓ No empty-string cells\")\n",
    "\n",
    "# Duplicates\n",
    "dup_count = int(df.duplicated().sum())\n",
    "print(f\"\\nExact duplicate rows: {dup_count}\")\n",
    "if dup_count > 0:\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    print(f\"  Dropped {dup_count} rows. New shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"  ✓ No duplicate rows\")\n",
    "\n",
    "# Negative death values (impossible)\n",
    "num_cols   = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "neg_counts = (df[num_cols] < 0).sum()\n",
    "if (neg_counts > 0).any():\n",
    "    print(\"\\n⚠ Negative values found:\"); display(neg_counts[neg_counts > 0])\n",
    "else:\n",
    "    print(\"  ✓ No negative values in numeric columns\")\n",
    "\n",
    "print(f\"\\nFinal shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s4",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 — Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-dict",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>dtype</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_pct</th>\n",
       "      <th>n_unique</th>\n",
       "      <th>sample_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>continent</td>\n",
       "      <td>str</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>Asia; Europe; Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>country</td>\n",
       "      <td>str</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>207</td>\n",
       "      <td>Afghanistan; Albania; Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deaths_direct_alcohol</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1288</td>\n",
       "      <td>72; 10; 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deaths_direct_drug</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>882</td>\n",
       "      <td>93; 5; 171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deaths_risk_alcohol</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3815</td>\n",
       "      <td>356; 289; 496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deaths_risk_drug</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1979</td>\n",
       "      <td>174; 64; 296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deaths_risk_tobacco</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4764</td>\n",
       "      <td>9723; 3468; 17985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>year</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30</td>\n",
       "      <td>1990; 1991; 1992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  column  dtype  missing_count  missing_pct  n_unique  \\\n",
       "0              continent    str              0         0.00         6   \n",
       "1                country    str              0         0.00       207   \n",
       "2  deaths_direct_alcohol  int64              0         0.00      1288   \n",
       "3     deaths_direct_drug  int64              0         0.00       882   \n",
       "4    deaths_risk_alcohol  int64              0         0.00      3815   \n",
       "5       deaths_risk_drug  int64              0         0.00      1979   \n",
       "6    deaths_risk_tobacco  int64              0         0.00      4764   \n",
       "7                   year  int64              0         0.00        30   \n",
       "\n",
       "                   sample_values  \n",
       "0           Asia; Europe; Africa  \n",
       "1  Afghanistan; Albania; Algeria  \n",
       "2                     72; 10; 75  \n",
       "3                     93; 5; 171  \n",
       "4                  356; 289; 496  \n",
       "5                   174; 64; 296  \n",
       "6              9723; 3468; 17985  \n",
       "7               1990; 1991; 1992  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [4] DATA DICTIONARY — schema overview with sample values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def data_dictionary(df_: pd.DataFrame, sample_n: int = 3) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for col in df_.columns:\n",
    "        s = df_[col]\n",
    "        rows.append({\n",
    "            \"column\"       : col,\n",
    "            \"dtype\"        : str(s.dtype),\n",
    "            \"missing_count\": int(s.isna().sum()),\n",
    "            \"missing_pct\"  : round(100 * s.isna().mean(), 2),\n",
    "            \"n_unique\"     : int(s.nunique(dropna=True)),\n",
    "            \"sample_values\": \"; \".join(map(lambda x: str(x)[:50], s.dropna().unique()[:sample_n])),\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(\"column\").reset_index(drop=True)\n",
    "\n",
    "dict_df = data_dictionary(df, sample_n=3)\n",
    "display(dict_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s5",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 — Standardisation & Shared Utilities\n",
    "\n",
    "All shared constants, colour maps, and helper functions are defined **once here** and referenced throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-standardise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns : ['continent', 'country', 'year', 'deaths_direct_drug', 'deaths_direct_alcohol', 'deaths_risk_tobacco', 'deaths_risk_drug', 'deaths_risk_alcohol', 'date', 'deaths_direct_total']\n",
      "Date range: 1990 → 2019\n",
      "\n",
      "Shared constants ready:\n",
      "  MEASURES (5): ['deaths_direct_drug', 'deaths_direct_alcohol', 'deaths_risk_tobacco', 'deaths_risk_drug', 'deaths_risk_alcohol']\n",
      "  PRIMARY_MEASURE: deaths_direct_total\n"
     ]
    }
   ],
   "source": [
    "# [5] STANDARDISATION + SHARED UTILITIES\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Everything defined here is used by all downstream cells.\n",
    "# DO NOT redefine MEASURES / PRETTY / MEASURE_COLORS elsewhere.\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ── 5.1 Column normalisation ───────────────────────────────────────────────────\n",
    "def to_snake(s: str) -> str:\n",
    "    return s.replace(\"/\",\" \").replace(\"-\",\" \").replace(\"(\",\" \").replace(\")\",\" \").replace(\":\",\" \")\n",
    "\n",
    "df.columns = (df.columns.map(str).map(to_snake)\n",
    "               .str.lower().str.replace(r\"[^a-z0-9]+\", \"_\", regex=True).str.strip(\"_\"))\n",
    "df = df.rename(columns={\"entity\": \"country\", \"code\": \"iso3\"})\n",
    "\n",
    "# ── 5.2 Date handling ─────────────────────────────────────────────────────────\n",
    "if \"date\" in df.columns:\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "elif \"year\" in df.columns:\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"year\"].astype(\"string\") + \"-01-01\", errors=\"coerce\")\n",
    "\n",
    "# ── 5.3 Numeric coercion ──────────────────────────────────────────────────────\n",
    "DIM_COLS = {\"country\", \"continent\", \"region\", \"iso3\", \"date\", \"year\"}\n",
    "for c in df.columns:\n",
    "    if c not in DIM_COLS and df[c].dtype == \"object\":\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# ── 5.4 Tidy categoricals ─────────────────────────────────────────────────────\n",
    "for c in [\"country\", \"continent\", \"region\", \"iso3\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# ── 5.5 Derived column: direct total ──────────────────────────────────────────\n",
    "if {\"deaths_direct_drug\", \"deaths_direct_alcohol\"}.issubset(df.columns):\n",
    "    df[\"deaths_direct_total\"] = df[\"deaths_direct_drug\"] + df[\"deaths_direct_alcohol\"]\n",
    "\n",
    "# ── 5.6 SHARED CONSTANTS ──────────────────────────────────────────────────────\n",
    "MEASURES = [\n",
    "    \"deaths_direct_drug\",\n",
    "    \"deaths_direct_alcohol\",\n",
    "    \"deaths_risk_tobacco\",\n",
    "    \"deaths_risk_drug\",\n",
    "    \"deaths_risk_alcohol\",\n",
    "]\n",
    "PRETTY = {\n",
    "    \"deaths_direct_drug\"   : \"Direct: Drug\",\n",
    "    \"deaths_direct_alcohol\": \"Direct: Alcohol\",\n",
    "    \"deaths_risk_tobacco\"  : \"Risk: Tobacco\",\n",
    "    \"deaths_risk_drug\"     : \"Risk: Drug\",\n",
    "    \"deaths_risk_alcohol\"  : \"Risk: Alcohol\",\n",
    "    \"deaths_direct_total\"  : \"Direct Total (Drug + Alcohol)\",\n",
    "}\n",
    "MEASURE_COLORS = {\n",
    "    \"deaths_direct_drug\"   : \"#e74c3c\",\n",
    "    \"deaths_direct_alcohol\": \"#3498db\",\n",
    "    \"deaths_risk_tobacco\"  : \"#2ecc71\",\n",
    "    \"deaths_risk_drug\"     : \"#f39c12\",\n",
    "    \"deaths_risk_alcohol\"  : \"#8e44ad\",\n",
    "    \"deaths_direct_total\"  : \"#1abc9c\",\n",
    "}\n",
    "PRIMARY_MEASURE = \"deaths_direct_total\"\n",
    "\n",
    "# ── 5.7 SHARED HELPER FUNCTIONS ───────────────────────────────────────────────\n",
    "\n",
    "def safe_save_csv(df_: pd.DataFrame, path, overwrite: bool = True) -> Path:\n",
    "    \"\"\"Save DataFrame to CSV. Overwrites by default for clean re-runs.\"\"\"\n",
    "    path = Path(path)\n",
    "    df_.to_csv(path, index=False)\n",
    "    print(f\"  Saved → {path.name}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def safe_save_fig(fig, path) -> Path:\n",
    "    \"\"\"Save figure. Overwrites by default.\"\"\"\n",
    "    path = Path(path)\n",
    "    fig.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"  Saved → {path.name}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def bar_palette(n: int):\n",
    "    \"\"\"Return n distinct colours from tab20.\"\"\"\n",
    "    return [plt.cm.get_cmap(\"tab20\")(i) for i in np.linspace(0, 1, max(n, 2))]\n",
    "\n",
    "\n",
    "def top_n_all_years(dfx: pd.DataFrame, level: str, measure: str, n: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Sum a measure over all years, return top N entities with share %.\"\"\"\n",
    "    out = (dfx.dropna(subset=[level])\n",
    "              .groupby(level, as_index=False)[measure].sum()\n",
    "              .sort_values(measure, ascending=False).head(n))\n",
    "    out[\"share_pct\"] = out[measure] / dfx[measure].sum() * 100\n",
    "    return out.rename(columns={level: \"name\", measure: \"value\"}).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def regression_metrics(y_true, y_pred) -> dict:\n",
    "    \"\"\"Return MAE, RMSE, and MAPE.\"\"\"\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    mae  = float(np.mean(np.abs(y_true - y_pred)))\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / np.maximum(1e-9, np.abs(y_true)))) * 100)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE\": mape}\n",
    "\n",
    "\n",
    "# Verify\n",
    "print(\"Columns :\", df.columns.tolist())\n",
    "print(\"Date range:\", df['date'].min().year, \"→\", df['date'].max().year)\n",
    "print(\"\\nShared constants ready:\")\n",
    "print(f\"  MEASURES ({len(MEASURES)}):\", MEASURES)\n",
    "print(f\"  PRIMARY_MEASURE: {PRIMARY_MEASURE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-commit-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pushed: 'feat: data loading, quality checks, and standardisation (Sections 1–5)'\n",
      "   → https://github.com/Udodirim/substance-abuse-analytics\n"
     ]
    }
   ],
   "source": [
    "# ── COMMIT 1: Data loading, quality checks, standardisation ──────────────────\n",
    "git_commit_push(\"feat: data loading, quality checks, and standardisation (Sections 1–5)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s6",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 — Key Performance Indicators (KPIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-kpi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6] KPIs — global totals, YoY%, 3-yr rolling avg, per measure\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "def build_kpis(df_: pd.DataFrame, measures: list) -> pd.DataFrame:\n",
    "    annual = df_.groupby(\"date\", as_index=True)[measures].sum().sort_index()\n",
    "    rows = []\n",
    "    for m in measures:\n",
    "        s = annual[m].dropna()\n",
    "        tot   = float(s.sum())\n",
    "        last  = float(s.iloc[-1]) if len(s) else np.nan\n",
    "        yoy   = float((s.iloc[-1] - s.iloc[-2]) / s.iloc[-2] * 100) if len(s) >= 2 and s.iloc[-2] != 0 else np.nan\n",
    "        roll3 = float(s.rolling(3).mean().iloc[-1]) if len(s) >= 3 else np.nan\n",
    "        rows.append({\n",
    "            \"measure\"               : PRETTY.get(m, m),\n",
    "            \"period\"                : f\"{s.index.min().year}–{s.index.max().year}\",\n",
    "            \"total_all_years\"       : tot,\n",
    "            \"last_year\"             : s.index[-1].year,\n",
    "            \"last_year_value\"       : last,\n",
    "            \"yoy_change_pct\"        : round(yoy, 2) if not np.isnan(yoy) else None,\n",
    "            \"3yr_rolling_avg\"       : roll3,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "ALL_MEASURES = MEASURES + ([\"deaths_direct_total\"] if \"deaths_direct_total\" in df.columns else [])\n",
    "kpi_df = build_kpis(df, ALL_MEASURES)\n",
    "display(kpi_df.style.format({\n",
    "    \"total_all_years\": \"{:,.0f}\",\n",
    "    \"last_year_value\": \"{:,.0f}\",\n",
    "    \"3yr_rolling_avg\": \"{:,.0f}\",\n",
    "    \"yoy_change_pct\" : \"{:.2f}%\",\n",
    "}))\n",
    "\n",
    "safe_save_csv(kpi_df, PROJECT_DIR / \"kpi_all.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s7",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7 — EDA Visualisations\n",
    "\n",
    "Four key charts:  \n",
    "1. Global trend (all measures)  \n",
    "2. Top 15 countries in the final year  \n",
    "3. Top 5 countries & continents per measure (sum 1990–2019)  \n",
    "4. Composition pie + bar (full period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eda-trend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7A] GLOBAL TREND — all measures on one chart\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "annual = (df.groupby(\"date\", as_index=False)[MEASURES].sum()\n",
    "            .sort_values(\"date\").reset_index(drop=True))\n",
    "annual[\"year\"] = annual[\"date\"].dt.year\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "for m in MEASURES:\n",
    "    ax.plot(annual[\"date\"], annual[m], marker=\".\", linewidth=1.5,\n",
    "            label=PRETTY[m], color=MEASURE_COLORS[m])\n",
    "ax.set_title(\"Global Substance Abuse Mortality Trend (1990–2019)\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Deaths\")\n",
    "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f\"{x/1e6:.1f}M\"))\n",
    "ax.legend(loc=\"upper left\", framealpha=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "safe_save_fig(fig, FIG_DIR / \"01_trend_global_all_measures.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eda-top15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7B] TOP 15 COUNTRIES — last year in dataset\n",
    "\n",
    "EXCLUDE = {\"World\",\"High-income\",\"Upper-middle-income\",\"Lower-middle-income\",\"Low-income\"}\n",
    "df_c   = df[~df[\"country\"].isin(EXCLUDE)].copy()\n",
    "last_yr = int(df[\"date\"].dt.year.max())\n",
    "\n",
    "last_df = df_c[df_c[\"date\"].dt.year == last_yr]\n",
    "by_cty  = (last_df.groupby(\"country\", as_index=False)[\"deaths_direct_total\"]\n",
    "                   .sum()\n",
    "                   .sort_values(\"deaths_direct_total\", ascending=False)\n",
    "                   .head(15))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "colors  = bar_palette(len(by_cty))\n",
    "ax.barh(by_cty[\"country\"][::-1].values, by_cty[\"deaths_direct_total\"][::-1].values, color=colors)\n",
    "for i, (val, row) in enumerate(zip(by_cty[\"deaths_direct_total\"][::-1], by_cty[::-1].itertuples())):\n",
    "    ax.text(val, i, f\"  {val:,.0f}\", va=\"center\", fontsize=8)\n",
    "ax.set_title(f\"Top 15 Countries — Direct Substance Deaths ({last_yr})\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Deaths\")\n",
    "ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f\"{x/1e3:.0f}K\"))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "safe_save_fig(fig, FIG_DIR / f\"02_top15_countries_{last_yr}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eda-top5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7C] TOP 5 COUNTRIES & CONTINENTS PER MEASURE (sum 1990–2019)\n",
    "\n",
    "for m in MEASURES:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle(f\"{PRETTY[m]} — Sum 1990–2019\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    for ax, level in zip(axes, [\"country\", \"continent\"]):\n",
    "        if level not in df.columns:\n",
    "            ax.axis(\"off\"); ax.set_title(f\"No '{level}' column\"); continue\n",
    "        t = top_n_all_years(df_c if level == \"country\" else df, level, m, n=5)\n",
    "        ax.barh(t[\"name\"][::-1], t[\"value\"][::-1], color=bar_palette(5))\n",
    "        for i, (val, pc) in enumerate(zip(t[\"value\"][::-1], t[\"share_pct\"][::-1])):\n",
    "            ax.text(val, i, f\"  {val:,.0f} ({pc:.1f}%)\", va=\"center\", fontsize=8)\n",
    "        ax.set_title(f\"Top 5 {level.title()}s\")\n",
    "        ax.set_xlabel(\"Deaths\")\n",
    "        ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    safe_save_fig(fig, FIG_DIR / f\"03_top5_{m}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eda-comp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7D] COMPOSITION — pie + bar (full 1990–2019 period)\n",
    "\n",
    "comp   = df[MEASURES].sum(numeric_only=True)\n",
    "labels = [m for m in MEASURES if comp.get(m, 0) > 0]\n",
    "vals   = np.array([float(comp[m]) for m in labels])\n",
    "pct    = 100 * vals / vals.sum()\n",
    "\n",
    "wedge_colors  = [MEASURE_COLORS[l] for l in labels]\n",
    "explode       = [0.06 if p < 3 else 0 for p in pct]\n",
    "legend_labels = [f\"{PRETTY.get(l, l)} — {p:.1f}%\" for l, p in zip(labels, pct)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle(\"Mortality Composition by Cause — Sum 1990–2019\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# Pie\n",
    "wedges, _ = axes[0].pie(vals, labels=None, autopct=None,\n",
    "                        explode=explode, startangle=90, colors=wedge_colors)\n",
    "axes[0].axis(\"equal\")\n",
    "axes[0].legend(wedges, legend_labels, title=\"Cause\",\n",
    "               loc=\"center left\", bbox_to_anchor=(1.0, 0.5), frameon=False, fontsize=9)\n",
    "\n",
    "# Horizontal bar\n",
    "order    = np.argsort(vals)\n",
    "bar_lbls = [PRETTY.get(labels[i], labels[i]) for i in order]\n",
    "bar_cols = [MEASURE_COLORS[labels[i]] for i in order]\n",
    "axes[1].barh(bar_lbls, vals[order], color=bar_cols)\n",
    "for i, (v, p) in enumerate(zip(vals[order], pct[order])):\n",
    "    axes[1].text(v, i, f\"  {v:,.0f}  ({p:.1f}%)\", va=\"center\", fontsize=8)\n",
    "axes[1].set_xlabel(\"Deaths\")\n",
    "axes[1].xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f\"{x/1e9:.1f}B\"))\n",
    "axes[1].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "safe_save_fig(fig, FIG_DIR / \"04_composition_1990_2019.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-commit-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── COMMIT 2: KPIs and EDA visualisations ────────────────────────────────────\n",
    "git_commit_push(\"feat: KPIs and EDA visualisations (Sections 6–7)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s8",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8 — Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [8] FEATURE ENGINEERING — annual global series + lag/rolling features for modelling\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "# Global annual series for primary measure\n",
    "y = (df.groupby(\"date\", as_index=True)[PRIMARY_MEASURE]\n",
    "        .sum()\n",
    "        .sort_index())\n",
    "y.name = \"y\"\n",
    "\n",
    "# Feature frame\n",
    "fe = y.to_frame().copy()\n",
    "fe[\"year\"]  = fe.index.year\n",
    "fe[\"lag1\"]  = fe[\"y\"].shift(1)\n",
    "fe[\"lag2\"]  = fe[\"y\"].shift(2)\n",
    "fe[\"roll3\"] = fe[\"y\"].rolling(3, min_periods=3).mean()   # strict: NaN until 3rd obs\n",
    "fe[\"trend\"] = np.arange(len(fe))                          # linear time index\n",
    "\n",
    "fe_model = fe.dropna(subset=[\"lag1\", \"lag2\", \"roll3\"]).copy()\n",
    "\n",
    "print(f\"Full series : {len(fe)} years ({fe.index.min().year}–{fe.index.max().year})\")\n",
    "print(f\"Model-ready : {len(fe_model)} years (dropped {len(fe)-len(fe_model)} for warm-up)\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(fe_model.head())\n",
    "print(\"\\nLast 5 rows:\")\n",
    "display(fe_model.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-commit-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── COMMIT 3: Feature engineering ────────────────────────────────────────────\n",
    "git_commit_push(\"feat: feature engineering with lag and rolling window features (Section 8)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s9",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9 — Modelling & Forecasting\n",
    "\n",
    "Three candidate models are trained and evaluated on a 3-year hold-out test set:  \n",
    "- **SARIMAX** — grid search over (p,d,q) orders, selected by AIC  \n",
    "- **Prophet** — Facebook's additive decomposition model (configured correctly for annual data)  \n",
    "- **AutoTS** — automated ensemble selector  \n",
    "\n",
    "Winner is chosen by lowest RMSE, then refitted on the full series to produce a 5-year forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-model-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9A] MODEL TRAINING & EVALUATION — SARIMAX vs Prophet vs AutoTS\n",
    "\n",
    "import json, numpy as np, pandas as pd, warnings\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "H          = 5   # forecast horizon (years)\n",
    "TEST_YEARS = 3   # hold-out period\n",
    "\n",
    "def split_series(series, test_periods=3):\n",
    "    series = series.dropna()\n",
    "    test_periods = min(test_periods, max(1, len(series) // 4))\n",
    "    return series.iloc[:-test_periods], series.iloc[-test_periods:]\n",
    "\n",
    "y_train, y_test = split_series(y, TEST_YEARS)\n",
    "print(f\"Train: {y_train.index.min().year}–{y_train.index.max().year}  ({len(y_train)} obs)\")\n",
    "print(f\"Test : {y_test.index.min().year}–{y_test.index.max().year}   ({len(y_test)} obs)\")\n",
    "\n",
    "candidates = []\n",
    "\n",
    "# ── SARIMAX: grid over (p,d,q) ────────────────────────────────────────────────\n",
    "print(\"\\n[1/3] Fitting SARIMAX grid...\")\n",
    "best_sarimax = (None, np.inf, None)\n",
    "for p in (0, 1, 2):\n",
    "    for d in (0, 1):\n",
    "        for q in (0, 1, 2):\n",
    "            try:\n",
    "                fit = SARIMAX(y_train, order=(p, d, q),\n",
    "                              enforce_stationarity=False,\n",
    "                              enforce_invertibility=False).fit(disp=False)\n",
    "                if fit.aic < best_sarimax[1]:\n",
    "                    best_sarimax = ((p, d, q), fit.aic, fit)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "sarimax_order, sarimax_aic, sarimax_fit = best_sarimax\n",
    "if sarimax_fit:\n",
    "    pred = sarimax_fit.get_forecast(steps=len(y_test)).predicted_mean\n",
    "    m    = regression_metrics(y_test.values, pred.values)\n",
    "    candidates.append({\"model\": \"SARIMAX\", \"params\": {\"order\": sarimax_order, \"AIC\": round(sarimax_aic, 2)}, \"metrics\": m})\n",
    "    print(f\"  Best order: {sarimax_order}  AIC: {sarimax_aic:.2f}  MAPE: {m['MAPE']:.2f}%\")\n",
    "\n",
    "# ── Prophet: configured for annual data ──────────────────────────────────────\n",
    "# NOTE: with annual (YS) frequency, sub-annual seasonalities are meaningless.\n",
    "# yearly_seasonality is also disabled — 1 obs/year means no intra-year cycle to fit.\n",
    "if PROPHET_OK:\n",
    "    print(\"\\n[2/3] Fitting Prophet...\")\n",
    "    try:\n",
    "        df_tr = y_train.reset_index().rename(columns={\"date\": \"ds\", PRIMARY_MEASURE: \"y\"})\n",
    "        m_prophet = Prophet(\n",
    "            seasonality_mode=\"additive\",\n",
    "            yearly_seasonality=False,    # ← correct for annual data\n",
    "            weekly_seasonality=False,\n",
    "            daily_seasonality=False,\n",
    "            changepoint_prior_scale=0.3,\n",
    "            n_changepoints=min(10, len(df_tr) - 2)  # ← guard against n_changepoints > n_obs\n",
    "        )\n",
    "        m_prophet.fit(df_tr)\n",
    "        future = pd.DataFrame({\"ds\": y_test.index})\n",
    "        fc     = m_prophet.predict(future)\n",
    "        m      = regression_metrics(y_test.values, fc[\"yhat\"].values)\n",
    "        candidates.append({\"model\": \"Prophet\", \"params\": {\"changepoint_prior\": 0.3}, \"metrics\": m})\n",
    "        print(f\"  MAPE: {m['MAPE']:.2f}%\")\n",
    "    except Exception as e:\n",
    "        PROPHET_OK = False\n",
    "        print(f\"  Prophet failed: {e}\")\n",
    "\n",
    "# ── AutoTS ────────────────────────────────────────────────────────────────────\n",
    "if AUTOTS_OK:\n",
    "    print(\"\\n[3/3] Fitting AutoTS...\")\n",
    "    try:\n",
    "        at = AutoTS(\n",
    "            forecast_length=len(y_test), frequency=\"infer\",\n",
    "            model_list=\"fast\", transformer_list=\"fast\",\n",
    "            ensemble=\"simple\", max_generations=3,\n",
    "            num_validations=2, random_seed=42, verbose=0\n",
    "        ).fit(df=y_train.to_frame(name=\"value\"),\n",
    "              date_col=y_train.index.name, value_col=\"value\", id_col=None)\n",
    "        autots_pred = at.predict().forecast[\"value\"].values\n",
    "        m = regression_metrics(y_test.values, autots_pred)\n",
    "        candidates.append({\"model\": \"AutoTS(fast)\", \"params\": \"auto\", \"metrics\": m})\n",
    "        print(f\"  MAPE: {m['MAPE']:.2f}%\")\n",
    "    except Exception as e:\n",
    "        AUTOTS_OK = False\n",
    "        print(f\"  AutoTS failed: {e}\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-model-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9B] MODEL COMPARISON TABLE — clear display of all candidates\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "def choose_best(cands: list) -> dict:\n",
    "    valid = [c for c in cands if c[\"metrics\"] is not None]\n",
    "    return min(valid, key=lambda c: (c[\"metrics\"][\"RMSE\"], c[\"metrics\"][\"MAE\"])) if valid else None\n",
    "\n",
    "winner = choose_best(candidates)\n",
    "\n",
    "# Build display table\n",
    "rows = []\n",
    "for c in candidates:\n",
    "    if c[\"metrics\"]:\n",
    "        rows.append({\n",
    "            \"Model\"   : c[\"model\"],\n",
    "            \"MAE\"     : c[\"metrics\"][\"MAE\"],\n",
    "            \"RMSE\"    : c[\"metrics\"][\"RMSE\"],\n",
    "            \"MAPE (%)\" : round(c[\"metrics\"][\"MAPE\"], 2),\n",
    "            \"Winner\"  : \"✅\" if winner and c[\"model\"] == winner[\"model\"] else \"\"\n",
    "        })\n",
    "\n",
    "eval_df = pd.DataFrame(rows).sort_values(\"RMSE\").reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"  MODEL EVALUATION SUMMARY (hold-out test set)\")\n",
    "print(f\"  Test period: {y_test.index.min().year}–{y_test.index.max().year}  ({len(y_test)} years)\")\n",
    "print(\"=\" * 55)\n",
    "display(eval_df.style\n",
    "    .format({\"MAE\": \"{:,.0f}\", \"RMSE\": \"{:,.0f}\", \"MAPE (%)\": \"{:.2f}\"})\n",
    "    .highlight_min(subset=[\"RMSE\"], color=\"#d4edda\")\n",
    ")\n",
    "\n",
    "if winner:\n",
    "    print(f\"\\n🏆 Winner: {winner['model']}\")\n",
    "    print(f\"   MAPE = {winner['metrics']['MAPE']:.2f}%  |  \"\n",
    "          f\"RMSE = {winner['metrics']['RMSE']:,.0f}  |  \"\n",
    "          f\"MAE = {winner['metrics']['MAE']:,.0f}\")\n",
    "else:\n",
    "    print(\"No valid model fitted. Check library installations.\")\n",
    "\n",
    "# Save eval to CSV\n",
    "safe_save_csv(eval_df, PROJECT_DIR / \"model_evaluation.csv\")\n",
    "\n",
    "# Save full report JSON\n",
    "report = {\"primary_measure\": PRIMARY_MEASURE, \"test_years\": TEST_YEARS,\n",
    "          \"candidates\": candidates, \"winner\": winner}\n",
    "with open(PROJECT_DIR / \"model_report.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "print(\"  Saved → model_report.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-forecast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9C] MULTI-MEASURE FORECASTS — SARIMAX per measure, 5-year horizon\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "year_max   = int(df[\"date\"].dt.year.max())\n",
    "future_idx = pd.date_range(\n",
    "    start=pd.Timestamp(year_max, 1, 1) + pd.offsets.YearBegin(1),\n",
    "    periods=H, freq=\"YS\"\n",
    ")\n",
    "\n",
    "def best_sarimax_fit(series: pd.Series):\n",
    "    \"\"\"AIC grid search over (p,d,q); returns (order, fitted_model).\"\"\"\n",
    "    series = series.dropna()\n",
    "    best   = (None, np.inf, None)\n",
    "    for p in (0, 1, 2):\n",
    "        for d in (0, 1):\n",
    "            for q in (0, 1, 2):\n",
    "                try:\n",
    "                    fit = SARIMAX(series, order=(p, d, q),\n",
    "                                  enforce_stationarity=False,\n",
    "                                  enforce_invertibility=False).fit(disp=False)\n",
    "                    if fit.aic < best[1]:\n",
    "                        best = ((p, d, q), fit.aic, fit)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    if best[2] is None:\n",
    "        fit = SARIMAX(series, order=(0, 1, 1),\n",
    "                      enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "        return (0, 1, 1), fit\n",
    "    return best[0], best[2]\n",
    "\n",
    "all_forecasts = []\n",
    "order_report  = []\n",
    "\n",
    "for m in MEASURES:\n",
    "    y_m = df.groupby(\"date\", as_index=True)[m].sum().sort_index()\n",
    "    if y_m.fillna(0).sum() == 0 or y_m.dropna().shape[0] < 8:\n",
    "        print(f\"Skipping {m}: insufficient data.\")\n",
    "        continue\n",
    "\n",
    "    order, fit = best_sarimax_fit(y_m)\n",
    "    f     = fit.get_forecast(steps=H)\n",
    "    fmean = f.predicted_mean\n",
    "    conf  = f.conf_int(alpha=0.2)\n",
    "\n",
    "    fc = pd.DataFrame({\n",
    "        \"date\"      : future_idx[:len(fmean)],\n",
    "        \"measure\"   : m,\n",
    "        \"prediction\": fmean.values[:len(future_idx)],\n",
    "        \"lower\"     : conf.iloc[:, 0].values[:len(future_idx)],\n",
    "        \"upper\"     : conf.iloc[:, 1].values[:len(future_idx)],\n",
    "    })\n",
    "    all_forecasts.append(fc)\n",
    "    order_report.append({\"measure\": m, \"sarimax_order\": order})\n",
    "    print(f\"  {PRETTY[m]:<30} order={order}\")\n",
    "\n",
    "forecast_long = pd.concat(all_forecasts, ignore_index=True).sort_values([\"date\", \"measure\"])\n",
    "forecast_wide = (forecast_long.pivot(index=\"date\", columns=\"measure\", values=\"prediction\")\n",
    "                 .reset_index().sort_values(\"date\"))\n",
    "\n",
    "print(\"\\nForecast preview:\")\n",
    "display(forecast_wide)\n",
    "\n",
    "safe_save_csv(forecast_long, PROJECT_DIR / \"forecast_by_measure_long.csv\")\n",
    "safe_save_csv(forecast_wide, PROJECT_DIR / \"forecast_by_measure_wide.csv\")\n",
    "with open(PROJECT_DIR / \"model_report_by_measure.json\", \"w\") as f:\n",
    "    json.dump(order_report, f, indent=2)\n",
    "print(\"  Saved → model_report_by_measure.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-forecast-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9D] FORECAST VISUALISATION — actual vs forecast with confidence intervals\n",
    "\n",
    "import matplotlib.pyplot as plt, matplotlib.ticker as mticker\n",
    "\n",
    "actuals_long = (df.groupby(\"date\", as_index=False)[MEASURES].sum()\n",
    "                  .melt(id_vars=\"date\", var_name=\"measure\", value_name=\"actual\"))\n",
    "\n",
    "combo = pd.merge(actuals_long, forecast_long, on=[\"date\", \"measure\"], how=\"outer\").sort_values([\"measure\", \"date\"])\n",
    "\n",
    "for m in MEASURES:\n",
    "    sub = combo[combo[\"measure\"] == m].sort_values(\"date\")\n",
    "    fig, ax = plt.subplots(figsize=(11, 4))\n",
    "\n",
    "    a = sub.dropna(subset=[\"actual\"])\n",
    "    f = sub.dropna(subset=[\"prediction\"])\n",
    "\n",
    "    if not a.empty:\n",
    "        ax.plot(a[\"date\"], a[\"actual\"], marker=\"o\", lw=1.5, label=\"Actual\",\n",
    "                color=MEASURE_COLORS[m])\n",
    "    if not f.empty:\n",
    "        ax.fill_between(f[\"date\"], f[\"lower\"], f[\"upper\"],\n",
    "                        alpha=0.15, color=MEASURE_COLORS[m], label=\"80% CI\")\n",
    "        ax.plot(f[\"date\"], f[\"prediction\"], marker=\"o\", ls=\"--\", lw=1.5,\n",
    "                color=MEASURE_COLORS[m], label=\"Forecast\")\n",
    "\n",
    "    # Shade forecast region\n",
    "    if not f.empty:\n",
    "        ax.axvline(x=a[\"date\"].max(), color=\"grey\", ls=\":\", lw=0.8)\n",
    "\n",
    "    ax.set_title(f\"Actual vs Forecast — {PRETTY[m]}\", fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Deaths\")\n",
    "    ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f\"{x/1e6:.2f}M\"))\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    safe_save_fig(fig, FIG_DIR / f\"05_forecast_{m}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-commit-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── COMMIT 4: Modelling and forecasting ──────────────────────────────────────\n",
    "git_commit_push(\"feat: SARIMAX/Prophet/AutoTS modelling, evaluation, and 5-year forecasts (Section 9)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s10",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10 — Exports\n",
    "\n",
    "All output files are saved in flat CSVs suitable for both **Power BI** and **Tableau**.\n",
    "\n",
    "| File | Description |\n",
    "|---|---|\n",
    "| `annual_totals.csv` | One row per year, global totals |\n",
    "| `country_year.csv` | Country × year with computed totals |\n",
    "| `measures_long.csv` | Annual data in long/tidy format |\n",
    "| `fact_actual_forecast.csv` | Actuals + forecasts combined |\n",
    "| `kpi_all.csv` | KPI summary table |\n",
    "| `model_evaluation.csv` | Model comparison metrics |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-export-core",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [10A] EXPORT — annual totals, country-year, long format\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ALL_M = MEASURES + ([\"deaths_direct_total\"] if \"deaths_direct_total\" in df.columns else [])\n",
    "\n",
    "# Annual global totals\n",
    "annual = (df.groupby(\"date\", as_index=False)[ALL_M].sum()\n",
    "            .sort_values(\"date\").reset_index(drop=True))\n",
    "annual[\"year\"] = annual[\"date\"].dt.year\n",
    "safe_save_csv(annual[[\"date\", \"year\"] + ALL_M], PROJECT_DIR / \"annual_totals.csv\")\n",
    "\n",
    "# Country × year\n",
    "group_cols  = [c for c in [\"continent\", \"country\", \"date\"] if c in df.columns]\n",
    "country_year = df.groupby(group_cols, as_index=False)[MEASURES].sum().sort_values(group_cols)\n",
    "country_year[\"deaths_direct_total\"] = country_year[\"deaths_direct_drug\"] + country_year[\"deaths_direct_alcohol\"]\n",
    "country_year[\"year\"]                = pd.to_datetime(country_year[\"date\"]).dt.year\n",
    "safe_save_csv(country_year, PROJECT_DIR / \"country_year.csv\")\n",
    "\n",
    "# Long / tidy format\n",
    "long_df = annual.melt(id_vars=[\"date\", \"year\"], value_vars=ALL_M,\n",
    "                      var_name=\"measure\", value_name=\"value\")\n",
    "long_df[\"measure_label\"] = long_df[\"measure\"].map(PRETTY)\n",
    "safe_save_csv(long_df, PROJECT_DIR / \"measures_long.csv\")\n",
    "\n",
    "print(\"\\n✓ Core export files saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-export-fact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [10B] FACT TABLE — actuals + forecasts combined (Power BI / Tableau ready)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "actuals_long = annual.melt(id_vars=[\"date\", \"year\"], value_vars=MEASURES,\n",
    "                            var_name=\"measure\", value_name=\"actual\")\n",
    "\n",
    "combo = pd.merge(actuals_long, forecast_long, on=[\"date\", \"measure\"], how=\"outer\")\n",
    "combo[\"year\"]        = combo[\"year\"].fillna(combo[\"date\"].dt.year).astype(int)\n",
    "combo[\"is_forecast\"] = combo[\"prediction\"].notna() & combo[\"actual\"].isna()\n",
    "combo[\"data_type\"]   = combo[\"is_forecast\"].map({True: \"Forecast\", False: \"Actual\"})\n",
    "combo[\"measure_label\"] = combo[\"measure\"].map(PRETTY)\n",
    "combo = combo.sort_values([\"measure\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "display(combo.head(10))\n",
    "safe_save_csv(combo, PROJECT_DIR / \"fact_actual_forecast.csv\")\n",
    "\n",
    "print(f\"\\nFact table: {len(combo)} rows, {combo['measure'].nunique()} measures\")\n",
    "print(f\"Years covered: {combo['year'].min()}–{combo['year'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-export-tableau",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [10C] ENHANCED COUNTRY TABLE — decade grouping, direct/risk splits (no hardcoded populations)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cy = country_year.copy()\n",
    "\n",
    "# Computed columns\n",
    "cy[\"total_direct_deaths\"] = cy[\"deaths_direct_drug\"] + cy[\"deaths_direct_alcohol\"]\n",
    "cy[\"total_risk_deaths\"]   = cy[[\"deaths_risk_tobacco\", \"deaths_risk_drug\", \"deaths_risk_alcohol\"]].sum(axis=1)\n",
    "cy[\"total_deaths\"]        = cy[\"total_direct_deaths\"] + cy[\"total_risk_deaths\"]\n",
    "cy[\"data_type\"]           = \"Actual\"\n",
    "cy[\"decade\"]              = (cy[\"year\"] // 10) * 10\n",
    "cy[\"decade_label\"]        = cy[\"decade\"].astype(str) + \"s\"\n",
    "\n",
    "# Year-over-year change per country (total_deaths)\n",
    "cy = cy.sort_values([\"country\", \"year\"])\n",
    "cy[\"total_deaths_yoy_pct\"] = (cy.groupby(\"country\")[\"total_deaths\"]\n",
    "                                  .pct_change() * 100).round(2)\n",
    "\n",
    "tableau_dir = PROJECT_DIR / \"tableau_ready\"\n",
    "tableau_dir.mkdir(exist_ok=True)\n",
    "\n",
    "safe_save_csv(cy, tableau_dir / \"country_year_enhanced.csv\")\n",
    "\n",
    "# Annual summary with 3yr moving avg + YoY\n",
    "ann_s = annual.copy()\n",
    "for col in ALL_M:\n",
    "    if col in ann_s.columns:\n",
    "        ann_s[f\"{col}_yoy_pct\"] = ann_s[col].pct_change() * 100\n",
    "        ann_s[f\"{col}_ma3\"]     = ann_s[col].rolling(window=3, center=True).mean()\n",
    "\n",
    "safe_save_csv(ann_s, tableau_dir / \"annual_summary_enhanced.csv\")\n",
    "\n",
    "# Combined global actual + forecast\n",
    "global_actual = cy.groupby(\"year\").agg(\n",
    "    deaths_direct_drug   = (\"deaths_direct_drug\",    \"sum\"),\n",
    "    deaths_direct_alcohol= (\"deaths_direct_alcohol\", \"sum\"),\n",
    "    deaths_risk_tobacco  = (\"deaths_risk_tobacco\",   \"sum\"),\n",
    "    deaths_risk_drug     = (\"deaths_risk_drug\",      \"sum\"),\n",
    "    deaths_risk_alcohol  = (\"deaths_risk_alcohol\",   \"sum\"),\n",
    "    total_deaths         = (\"total_deaths\",           \"sum\")\n",
    ").reset_index()\n",
    "global_actual[\"data_type\"] = \"Actual\"\n",
    "\n",
    "forecast_totals                = forecast_wide.copy()\n",
    "forecast_totals[\"year\"]        = pd.to_datetime(forecast_totals[\"date\"]).dt.year\n",
    "forecast_totals[\"total_deaths\"]= forecast_totals[MEASURES].sum(axis=1)\n",
    "forecast_totals[\"data_type\"]   = \"Forecast\"\n",
    "\n",
    "combined_global = pd.concat([\n",
    "    global_actual,\n",
    "    forecast_totals[[\"year\"] + MEASURES + [\"total_deaths\", \"data_type\"]]\n",
    "], ignore_index=True)\n",
    "\n",
    "safe_save_csv(combined_global, tableau_dir / \"global_actual_forecast.csv\")\n",
    "safe_save_csv(long_df,         tableau_dir / \"measures_long.csv\")\n",
    "\n",
    "print(\"\\n✓ All Tableau-ready files saved to:\", tableau_dir)\n",
    "print(\"Files:\")\n",
    "for f in sorted(tableau_dir.iterdir()):\n",
    "    print(f\"  {f.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-commit-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── COMMIT 5: Export files (Power BI and Tableau) ────────────────────────────\n",
    "git_commit_push(\"feat: Power BI and Tableau export files (Section 10)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s11",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11 — Data Quality Report & Conclusions\n",
    "\n",
    "Final validation pass before handing off to dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [11] FINAL DATA QUALITY REPORT + KEY FINDINGS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cy_check = pd.read_csv(tableau_dir / \"country_year_enhanced.csv\")\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"  FINAL DATA QUALITY REPORT\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "missing = cy_check.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "if len(missing):\n",
    "    print(\"\\n⚠ Missing values:\")\n",
    "    display(missing)\n",
    "else:\n",
    "    print(\"\\n  ✓ No missing values in enhanced country table\")\n",
    "\n",
    "print(f\"\\nCoverage  : {cy_check['year'].min()}–{cy_check['year'].max()}\")\n",
    "print(f\"Countries : {cy_check['country'].nunique()}\")\n",
    "print(f\"Rows      : {len(cy_check):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"  KEY FINDINGS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "top5_global = (cy_check[cy_check[\"year\"] == cy_check[\"year\"].max()]\n",
    "               .nlargest(5, \"total_deaths\")[[\"country\", \"total_deaths\", \"continent\"]])\n",
    "\n",
    "print(f\"\\nTop 5 countries by total substance deaths ({cy_check['year'].max()}):\")\n",
    "display(top5_global.reset_index(drop=True))\n",
    "\n",
    "# Global change 1990 → last year\n",
    "first_yr = int(cy_check[\"year\"].min())\n",
    "last_yr  = int(cy_check[\"year\"].max())\n",
    "g1 = cy_check[cy_check[\"year\"] == first_yr][\"total_deaths\"].sum()\n",
    "g2 = cy_check[cy_check[\"year\"] == last_yr][\"total_deaths\"].sum()\n",
    "pct_chg = (g2 - g1) / g1 * 100\n",
    "\n",
    "print(f\"\\nGlobal total deaths {first_yr}: {g1:,.0f}\")\n",
    "print(f\"Global total deaths {last_yr} : {g2:,.0f}\")\n",
    "print(f\"Change over period  : {'↑' if pct_chg > 0 else '↓'} {abs(pct_chg):.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"  ANALYSIS COMPLETE ✓\")\n",
    "print(\"=\" * 55)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-commit-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── COMMIT 6: Quality report and conclusions ──────────────────────────────────\n",
    "git_commit_push(\"feat: final data quality report and key findings (Section 11)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-readme",
   "metadata": {},
   "source": [
    "---\n",
    "## 📋 README Push\n",
    "\n",
    "This cell writes a professional README to the repo directory and pushes it as a final commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-readme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [12] WRITE README.md TO REPO + FINAL PUSH\n",
    "\n",
    "readme_content = \"\"\"# 🧪 Global Substance Abuse Mortality Analytics\n",
    "\n",
    "> **Portfolio Project** | Eden Mandate AI_Tools  \n",
    "> Master's in AI and Data Science\n",
    "\n",
    "## Overview\n",
    "End-to-end analysis of global mortality attributable to substance abuse (drug, alcohol, tobacco)  \n",
    "across 207 countries from 1990–2019, with a 5-year SARIMAX forecast through 2024.\n",
    "\n",
    "## Dataset\n",
    "- **Source:** Global Burden of Disease / Our World in Data  \n",
    "- **Shape:** 6,210 rows × 8 columns  \n",
    "- **Coverage:** 207 countries · 30 years (1990–2019)  \n",
    "- **Measures:** 5 death categories (direct drug, direct alcohol, risk tobacco, risk drug, risk alcohol)\n",
    "\n",
    "## Methodology\n",
    "\n",
    "| Step | Description |\n",
    "|---|---|\n",
    "| Data Quality | Null checks, duplicate detection, negative value validation |\n",
    "| Standardisation | Snake_case normalisation, date parsing, numeric coercion |\n",
    "| Feature Engineering | Lag features (t-1, t-2), 3-year rolling mean, trend index |\n",
    "| Modelling | SARIMAX (AIC grid), Prophet (annual-corrected), AutoTS (fast ensemble) |\n",
    "| Evaluation | Hold-out test set (3 years) · MAE, RMSE, MAPE |\n",
    "| Forecasting | 5-year horizon per measure with 80% confidence intervals |\n",
    "| Exports | Power BI and Tableau ready CSVs |\n",
    "\n",
    "## Key Results\n",
    "\n",
    "See `model_evaluation.csv` for full metrics. Best model selected by lowest RMSE on the 3-year hold-out.\n",
    "\n",
    "## Output Files\n",
    "\n",
    "```\n",
    "├── annual_totals.csv               # Global annual totals\n",
    "├── country_year.csv                # Country × year breakdowns\n",
    "├── measures_long.csv               # Tidy long format\n",
    "├── fact_actual_forecast.csv        # Actuals + forecasts combined\n",
    "├── forecast_by_measure_long.csv    # Forecasts in long format\n",
    "├── forecast_by_measure_wide.csv    # Forecasts in wide format\n",
    "├── kpi_all.csv                     # KPI summary table\n",
    "├── model_evaluation.csv            # Model comparison (MAE/RMSE/MAPE)\n",
    "├── model_report.json               # Full model selection report\n",
    "└── tableau_ready/                  # Tableau-optimised exports\n",
    "```\n",
    "\n",
    "## Tech Stack\n",
    "Python · pandas · statsmodels (SARIMAX) · Prophet · AutoTS · matplotlib · Google Colab\n",
    "\n",
    "## Commit Structure\n",
    "| Commit | Content |\n",
    "|---|---|\n",
    "| 1 | Data loading, quality checks, standardisation |\n",
    "| 2 | KPIs and EDA visualisations |\n",
    "| 3 | Feature engineering |\n",
    "| 4 | Modelling, evaluation, forecasting |\n",
    "| 5 | Export files |\n",
    "| 6 | Quality report and findings |\n",
    "| 7 | README |\n",
    "\n",
    "---\n",
    "*Built with integrity — real metrics, honest model reporting.*\n",
    "\"\"\"\n",
    "\n",
    "readme_path = Path(REPO_DIR) / \"README.md\"\n",
    "readme_path.write_text(readme_content)\n",
    "print(\"✓ README.md written\")\n",
    "\n",
    "git_commit_push(\"docs: add professional README with methodology and output index\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
